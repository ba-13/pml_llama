{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ba13/anaconda3/envs/conda-torch/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/ba13/anaconda3/envs/conda-torch/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.utils.convert_parameters import parameters_to_vector\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"../data\", train=True, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"../data\", train=False, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214538"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the network\n",
    "net = Net().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "# model parameters\n",
    "sum(p.numel() for p in net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_snapshots_total = 3\n",
    "snapshot_freq = 1\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 3e-4\n",
    "min_var = 1e-30\n",
    "_n_params_subnet = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [10000/60000], Loss: 1.4626\n",
      "Epoch [1/3], Step [20000/60000], Loss: 1.4612\n",
      "Epoch [1/3], Step [30000/60000], Loss: 1.4612\n",
      "Epoch [1/3], Step [40000/60000], Loss: 1.4612\n",
      "Epoch [1/3], Step [50000/60000], Loss: 1.4612\n",
      "Epoch [1/3], Step [60000/60000], Loss: 1.4612\n",
      "Epoch [2/3], Step [10000/60000], Loss: 1.4612\n",
      "Epoch [2/3], Step [20000/60000], Loss: 1.4612\n",
      "Epoch [2/3], Step [30000/60000], Loss: 2.4612\n",
      "Epoch [2/3], Step [40000/60000], Loss: 1.4612\n",
      "Epoch [2/3], Step [50000/60000], Loss: 1.4612\n",
      "Epoch [2/3], Step [60000/60000], Loss: 1.4612\n",
      "Epoch [3/3], Step [10000/60000], Loss: 1.4612\n",
      "Epoch [3/3], Step [20000/60000], Loss: 1.4612\n",
      "Epoch [3/3], Step [30000/60000], Loss: 1.4612\n",
      "Epoch [3/3], Step [40000/60000], Loss: 1.4612\n",
      "Epoch [3/3], Step [50000/60000], Loss: 1.4612\n",
      "Epoch [3/3], Step [60000/60000], Loss: 1.4612\n"
     ]
    }
   ],
   "source": [
    "# Train the network for 3 epochs\n",
    "net.train()\n",
    "for epoch in range(3):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(\n",
    "                \"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}\".format(\n",
    "                    epoch + 1, 3, i + 1, len(train_loader), loss.item()\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.05%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the network on the test set\n",
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(\"Test Accuracy: {}%\".format((correct / total) * 100))\n",
    "\n",
    "# Save the model\n",
    "torch.save(net.state_dict(), \"./mnist_net.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = Net().to(device)\n",
    "model.load_state_dict(torch.load(\"./mnist_net.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = deepcopy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _param_vector(model):\n",
    "    return parameters_to_vector(model.parameters()).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.zeros_like(_param_vector(_model))\n",
    "sq_mean = torch.zeros_like(_param_vector(_model))\n",
    "n_snapshots = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    _model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay\n",
    ")\n",
    "n_epochs = snapshot_freq * n_snapshots_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "epoch  1\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "epoch  2\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(\"epoch \", epoch)\n",
    "    i = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        i = i + 1\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(_model(inputs), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % snapshot_freq == 0:\n",
    "        old_fac, new_fac = n_snapshots / (n_snapshots + 1), 1 / (n_snapshots + 1)\n",
    "        mean = mean * old_fac + _param_vector(_model) * new_fac\n",
    "        sq_mean = sq_mean * old_fac + _param_vector(_model) ** 2 * new_fac\n",
    "        n_snapshots += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_variances = torch.clamp(sq_mean - mean**2, min_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.argsort(param_variances, descending=True)[:32]\n",
    "idx = idx.sort()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_vector = parameters_to_vector(net.parameters()).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "subnet_mask = torch.zeros_like(parameter_vector).bool()\n",
    "subnet_mask[idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subnet_mask_indices = subnet_mask.nonzero(as_tuple=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1908,   2199,   3637, 130844, 130845, 130846, 130851, 130853, 130858,\n",
       "        130859, 130860, 130861, 130872, 130873, 131425, 131426, 131427, 131433,\n",
       "        131434, 131435, 131440, 131441, 131442, 168567, 168574, 168580, 168583,\n",
       "        168587, 168590, 168597, 211394, 214189], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(subnet_mask_indices, \"subnet_mask_indices.pt\")\n",
    "subnet_mask_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subnet_mask_indices = torch.load(\"./subnet_mask_indices.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying on samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {k: v for k, v in net.named_parameters() if v.requires_grad}\n",
    "buffers_dict = {k: v for k, v in net.named_buffers()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_iter = iter(test_loader)\n",
    "x, label_var = next(data_iter)\n",
    "\n",
    "x = x[0]\n",
    "label = label_var[0]\n",
    "\n",
    "image = x.view(28, 28)\n",
    "\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.title(\"Label: {}\".format(label))\n",
    "plt.axis(\"off\")  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)\n",
    "params_dict = {key: value.to(device) for key, value in params_dict.items()}\n",
    "buffers_dict = {key: value.to(device) for key, value in buffers_dict.items()}\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_params_only(params_dict, buffers_dict):\n",
    "    out = torch.func.functional_call(net, (params_dict, buffers_dict), x)\n",
    "    return out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = len(parameters_to_vector(net.parameters()).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "n_data = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "mean = parameters_to_vector(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    try:\n",
    "        out = net(X[:1].to(device))\n",
    "    except (TypeError, AttributeError):\n",
    "        out = net(X.to(device))\n",
    "n_outputs = out.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(net, \"output_size\", n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = torch.zeros(_n_params_subnet, _n_params_subnet, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobians(x):\n",
    "    def model_fn_params_only(params_dict, buffers_dict):\n",
    "        out = torch.func.functional_call(net, (params_dict, buffers_dict), x)\n",
    "        return out, out\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Js, f = torch.func.jacrev(model_fn_params_only, has_aux=True)(\n",
    "            params_dict, buffers_dict\n",
    "        )\n",
    "\n",
    "    Js = [\n",
    "        j.flatten(start_dim=-p.dim()) for j, p in zip(Js.values(), params_dict.values())\n",
    "    ]\n",
    "    Js = torch.cat(Js, dim=-1)\n",
    "\n",
    "    Js = Js[:, :, subnet_mask_indices]\n",
    "    return Js, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "N = len(train_loader.dataset)\n",
    "i = 0\n",
    "for X, y in train_loader:\n",
    "    i = i + 1\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    net.zero_grad()\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    Js, f = jacobians(X)\n",
    "    ps = torch.softmax(f, dim=-1)\n",
    "    H_lik = torch.diag_embed(ps) - torch.einsum(\"mk,mc->mck\", ps, ps)\n",
    "    H_batch = torch.einsum(\"bcp,bck,bkq->pq\", Js, H_lik, Js)\n",
    "    lossfunc = CrossEntropyLoss(reduction=\"sum\")\n",
    "    loss_batch = 1.0 * lossfunc(f, y)\n",
    "    loss += loss_batch\n",
    "    H += H_batch\n",
    "    del X, y, H_lik, H_batch, loss_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_loader:\n",
    "    print(X.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.9244e-03, -1.3882e-03,  1.0609e-05,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [-1.3882e-03,  2.3567e-03,  2.0977e-05,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 1.0609e-05,  2.0977e-05,  3.4086e-04,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        ...,\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_noise = 1.0\n",
    "temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_noise = torch.tensor(sigma_noise, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma2 = sigma_noise.square()\n",
    "_H_factor = 1 / sigma2 / temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.multivariate_normal import _precision_to_scale_tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32])\n"
     ]
    }
   ],
   "source": [
    "prior_precision = 1.0\n",
    "prior_precision_diag = torch.ones(_n_params_subnet, device=device)\n",
    "posterior_precision = _H_factor * H + torch.diag(prior_precision_diag)\n",
    "# posterior_precision = torch.diag(prior_precision_diag)\n",
    "invsqrt_precision = _precision_to_scale_tril\n",
    "posterior_scale = invsqrt_precision(posterior_precision)\n",
    "scale = posterior_scale\n",
    "posterior_covariance = scale @ scale.T\n",
    "print(posterior_covariance.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_iter = iter(train_loader)\n",
    "x, label_var = next(data_iter)\n",
    "x = x[0]\n",
    "label = label_var[0]\n",
    "\n",
    "image = x.view(28, 28)\n",
    "\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.title(\"Label: {}\".format(label))\n",
    "plt.axis(\"off\")  # Hide axes\n",
    "plt.show()\n",
    "\n",
    "\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0853, 0.2320, 0.0853, 0.0853, 0.0853, 0.0853, 0.0853, 0.0853, 0.0853,\n",
      "         0.0853]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    Js, f_mu = torch.func.jacrev(model_fn_params_only, has_aux=True)(\n",
    "        params_dict, buffers_dict\n",
    "    )\n",
    "\n",
    "Js = [j.flatten(start_dim=-p.dim()) for j, p in zip(Js.values(), params_dict.values())]\n",
    "Js = torch.cat(Js, dim=-1)\n",
    "\n",
    "Js = Js[:, :, subnet_mask_indices]\n",
    "\n",
    "Js = Js.squeeze(0)\n",
    "f_var = torch.einsum(\"np,pq,mq->nm\", Js, posterior_covariance, Js)\n",
    "f_var = f_var.unsqueeze(0)\n",
    "kappa = 1 / torch.sqrt(1.0 + torch.pi / 8 * f_var.diagonal(dim1=1, dim2=2))\n",
    "final_ppd = torch.softmax(kappa * f_mu, dim=-1)\n",
    "print(final_ppd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
