{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ba13/anaconda3/envs/conda-torch/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/ba13/anaconda3/envs/conda-torch/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.utils.convert_parameters import parameters_to_vector\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214538"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the network\n",
    "net = Net().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "# model parameters\n",
    "sum(p.numel() for p in net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [10000/60000], Loss: 2.3152\n",
      "Epoch [1/3], Step [20000/60000], Loss: 2.2957\n",
      "Epoch [1/3], Step [30000/60000], Loss: 2.2950\n",
      "Epoch [1/3], Step [40000/60000], Loss: 2.2961\n",
      "Epoch [1/3], Step [50000/60000], Loss: 2.2869\n",
      "Epoch [1/3], Step [60000/60000], Loss: 2.2966\n"
     ]
    }
   ],
   "source": [
    "# Train the network for 3 epochs\n",
    "net.train()\n",
    "for epoch in range(1):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n",
    "                epoch + 1, 3, i + 1, len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 10.09%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the network on the test set\n",
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Test Accuracy: {}%'.format((correct / total) * 100))\n",
    "\n",
    "# Save the model\n",
    "torch.save(net.state_dict(), './mnist_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = Net().to(device)\n",
    "model.load_state_dict(torch.load(\"./mnist_net.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = deepcopy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _param_vector(model):\n",
    "    return parameters_to_vector(model.parameters()).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.zeros_like(_param_vector(_model))\n",
    "sq_mean = torch.zeros_like(_param_vector(_model))\n",
    "n_snapshots = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_snapshots_total = 3\n",
    "snapshot_freq = 1\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 3e-4\n",
    "min_var = 1e-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    _model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay\n",
    ")\n",
    "n_epochs = snapshot_freq * n_snapshots_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "epoch  1\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "epoch  2\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(\"epoch \", epoch)\n",
    "    i = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        i = i + 1\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(_model(inputs), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % snapshot_freq == 0:\n",
    "        old_fac, new_fac = n_snapshots / (n_snapshots + 1), 1 / (n_snapshots + 1)\n",
    "        mean = mean * old_fac + _param_vector(_model) * new_fac\n",
    "        sq_mean = sq_mean * old_fac + _param_vector(_model) ** 2 * new_fac\n",
    "        n_snapshots += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_variances = torch.clamp(sq_mean - mean**2, min_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_n_params_subnet = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.argsort(param_variances, descending=True)[:32]\n",
    "idx = idx.sort()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_vector = parameters_to_vector(net.parameters()).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "subnet_mask = torch.zeros_like(parameter_vector).bool()\n",
    "subnet_mask[idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subnet_mask_indices = subnet_mask.nonzero(as_tuple=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    54,     55,     56,     90,     91,     92,     93,     94,     98,\n",
       "           148,    150,    154,   1372,   2807,   2808,   2845,   2846,   2847,\n",
       "          2952,  24536,  88997,  88998,  89000, 171927, 171936, 172101, 172102,\n",
       "        172104, 190917, 190918, 210165, 214344], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(subnet_mask_indices, \"subnet_mask_indices.pt\")\n",
    "subnet_mask_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "subnet_mask_indices = torch.load(\"./subnet_mask_indices.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying on samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {k: v for k, v in net.named_parameters() if v.requires_grad}\n",
    "buffers_dict = {k: v for k, v in net.named_buffers()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_iter = iter(test_loader)\n",
    "x, label_var = next(data_iter)\n",
    "\n",
    "x = x[0]\n",
    "label = label_var[0]\n",
    "\n",
    "image = x.view(28, 28)\n",
    "\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.title(\"Label: {}\".format(label))\n",
    "plt.axis(\"off\")  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)\n",
    "params_dict = {key: value.to(device) for key, value in params_dict.items()}\n",
    "buffers_dict = {key: value.to(device) for key, value in buffers_dict.items()}\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_params_only(params_dict, buffers_dict):\n",
    "    out = torch.func.functional_call(net, (params_dict, buffers_dict), x)\n",
    "    return out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Js, f = torch.func.jacrev(model_fn_params_only, has_aux=True)(params_dict, buffers_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Js = [j.flatten(start_dim=-p.dim()) for j, p in zip(Js.values(), params_dict.values())]\n",
    "Js = torch.cat(Js, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Js = Js[:, :, subnet_mask_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mu = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch, n_outs, n_params = Js.shape\n",
    "Js = Js.reshape(n_batch * n_outs, n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = len(parameters_to_vector(net.parameters()).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = torch.zeros(_n_params_subnet, _n_params_subnet, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "n_data = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "mean = parameters_to_vector(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    try:\n",
    "        out = net(X[:1].to(device))\n",
    "    except (TypeError, AttributeError):\n",
    "        out = net(X.to(device))\n",
    "n_outputs = out.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(net, \"output_size\", n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobians(x):\n",
    "    def model_fn_params_only(params_dict, buffers_dict):\n",
    "        out = torch.func.functional_call(net, (params_dict, buffers_dict), x)\n",
    "        return out, out\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Js, f = torch.func.jacrev(model_fn_params_only, has_aux=True)(\n",
    "            params_dict, buffers_dict\n",
    "        )\n",
    "\n",
    "    Js = [\n",
    "        j.flatten(start_dim=-p.dim()) for j, p in zip(Js.values(), params_dict.values())\n",
    "    ]\n",
    "    Js = torch.cat(Js, dim=-1)\n",
    "\n",
    "    Js = Js[:, :, subnet_mask_indices]\n",
    "    return Js, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "N = len(train_loader.dataset)\n",
    "i = 0\n",
    "for X, y in train_loader:\n",
    "    i = i + 1\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    net.zero_grad()\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    Js, f = jacobians(X)\n",
    "    ps = torch.softmax(f, dim=-1)\n",
    "    H_lik = torch.diag_embed(ps) - torch.einsum(\"mk,mc->mck\", ps, ps)\n",
    "    H_batch = torch.einsum(\"bcp,bck,bkq->pq\", Js, H_lik, Js)\n",
    "    lossfunc = CrossEntropyLoss(reduction=\"sum\")\n",
    "    loss_batch = 1.0 * lossfunc(f, y)\n",
    "    loss += loss_batch\n",
    "    H += H_batch\n",
    "    del X, y, H_lik, H_batch, loss_batch\n",
    "n_data += N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_noise = 1.0\n",
    "temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_noise = torch.tensor(sigma_noise, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma2 = sigma_noise.square()\n",
    "_H_factor = 1 / sigma2 / temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.multivariate_normal import _precision_to_scale_tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(H, \"hessian.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32])\n"
     ]
    }
   ],
   "source": [
    "prior_precision = 1.\n",
    "prior_precision_diag = torch.ones(_n_params_subnet, device=device)\n",
    "posterior_precision = _H_factor * H + torch.diag(prior_precision_diag)\n",
    "# posterior_precision = torch.diag(prior_precision_diag)\n",
    "invsqrt_precision = _precision_to_scale_tril\n",
    "posterior_scale = invsqrt_precision(posterior_precision)\n",
    "scale = posterior_scale\n",
    "posterior_covariance = scale @ scale.T\n",
    "print(posterior_covariance.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_iter = iter(train_loader)\n",
    "x, label_var = next(data_iter)\n",
    "x = x[0]\n",
    "label = label_var[0]\n",
    "\n",
    "image = x.view(28, 28)\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('Label: {}'.format(label))\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n",
    "\n",
    "\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0988, 0.0985, 0.0998, 0.1007, 0.0998, 0.1006, 0.1011, 0.1002, 0.0990,\n",
      "         0.1016]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    Js, f_mu = torch.func.jacrev(model_fn_params_only, has_aux=True)(params_dict, buffers_dict)\n",
    "\n",
    "Js = [\n",
    "    j.flatten(start_dim=-p.dim())\n",
    "    for j, p in zip(Js.values(), params_dict.values())\n",
    "]\n",
    "Js = torch.cat(Js, dim=-1)\n",
    "\n",
    "Js = Js[:, :, subnet_mask_indices]\n",
    "\n",
    "Js = Js.squeeze(0)\n",
    "f_var = torch.einsum('np,pq,mq->nm', Js, posterior_covariance, Js)\n",
    "f_var = f_var.unsqueeze(0)\n",
    "kappa = 1 / torch.sqrt(1. + torch.pi / 8 * f_var.diagonal(dim1=1, dim2=2))\n",
    "final_ppd = torch.softmax(kappa * f_mu, dim=-1)\n",
    "print(final_ppd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
